#  iOSåº•å±‚Classç»“æ„ä¹‹Cache_t
åœ¨ä¹‹å‰çš„æ–‡ç« ä¸­æ¢ç´¢è¿‡ç±»çš„ç»“æ„çš„ç›¸å…³ä¿¡æ¯ï¼Œå…¶ä¸­ä¸»è¦è¯´äº† superclssã€isa ç­‰ï¼Œä½†æ˜¯è¿˜æœ‰ä¸€ä¸ª cache_tï¼Œè¿™ä¸ªä¹Ÿæ˜¯ Class ä¸­æå…¶é‡è¦çš„ä¸€éƒ¨åˆ†ã€‚

åœ¨ objc4-779 ä¹‹åçš„ç‰ˆæœ¬ï¼Œcache_t ç»“æ„å‘ç”Ÿäº†å˜åŒ–ï¼Œä½†æ˜¯åŸç†æ˜¯ä¸€æ ·çš„ã€‚
> ï¼ˆçœ‹åˆ°å¾ˆå¤šåšä¸»çš„çš„éƒ½æ˜¯å¯¹æ—§ç‰ˆçš„è§£æï¼Œåªæœ‰è¿™ä¸ªåšä¸»æ˜¯ç”¨çš„æœ€æ–°çš„ï¼Œæ‰€ä»¥ä¸€ç›´åœ¨çœ‹åšä¸»çš„æ–‡ç« ï¼Œæœ¬æ–‡ä¸­ç”¨çš„ä»£ç éƒ½æ˜¯ objc4-781 ä¸­çš„ï¼Œæ˜¯ç›®å‰è‹¹æœæœ€æ–°çš„ä»£ç ã€‚ï¼‰

cache_t  æ˜¯ç”¨æ¥ç¼“å­˜ class è°ƒç”¨è¿‡çš„å®ä¾‹æ–¹æ³•ã€‚ä½†æ˜¯å…¶æœ¬èº«çš„å·¥ä½œæœºåˆ¶å’Œæˆ‘ä»¬æƒ³è±¡çš„æ˜¯ä¸ä¸€æ ·çš„ã€‚
> æ–¹æ³•ç¼“å­˜åŸç†ï¼š
> 1. ç¼“å­˜ç¬¬ä¸€ä¸ªå®ä¾‹æ–¹æ³•çš„æ—¶å€™ï¼Œåˆå§‹åŒ–ä¸€ä¸ª mask å®¹é‡ä¸º 3 çš„è¡¨ï¼Œå¾€é‡Œé¢å­˜æ–¹æ³•ä¿¡æ¯å¹¶è®°å½•ç¼“å­˜æ•°é‡ä¸º 1.
> 2. å½“æœ‰æ–¹æ³•éœ€è¦ç¼“å­˜çš„æ—¶å€™ï¼Œå¦‚æœéœ€è¦ç¼“å­˜çš„æ–¹æ³•æ•°é‡ ï¼ˆå½“å‰ç¼“å­˜æ•°é‡ + 1ï¼‰<= å½“å‰å®¹é‡ + 1 çš„ 3/4, ç›´æ¥ç¼“å­˜å¹¶è®°å½•ç¼“å­˜æ•°é‡ + 1
> 3. å½“æœ‰æ–¹æ³•éœ€è¦ç¼“å­˜çš„æ—¶å€™ï¼Œå¦‚æœéœ€è¦ç¼“å­˜çš„æ–¹æ³•æ•°é‡ï¼ˆå½“å‰ç¼“å­˜æ•°é‡ + 1ï¼‰> å½“å‰å®¹é‡çš„ 3/4, æŒ‰ç…§ä¹‹å‰çš„å®¹é‡ x ( (x + 1) 2 - 1) é‡æ–°åˆ†é…å­˜å‚¨å®¹é‡ï¼Œå¹¶æ¸…é™¤ä¹‹å‰ç¼“å­˜çš„æ–¹æ³•ä¿¡æ¯ï¼Œç„¶åå†å­˜å‚¨ï¼Œå¹¶è®°å½•ç¼“å­˜æ•°é‡ä¸º 1.ï¼ˆæ‰©å®¹ä»¥åå¹¶ä¸ä¼šæŠŠä»¥å‰çš„æ•°æ®å¤åˆ¶åˆ°æ–°å†…å­˜ä¸­ï¼Œè€Œæ˜¯ç›´æ¥èˆå¼ƒï¼‰

> mask çš„æœºåˆ¶
> 1.  åˆå§‹åŒ–ä¸º 4 - 1
> 2. å½“éœ€è¦å­˜å‚¨æ•°é‡ > å½“å‰å®¹é‡ + 1 çš„ 3/4ï¼Œé‡æ–°åˆ†é…å®¹é‡ ï¼ˆï¼ˆå½“å‰å®¹é‡ + 1ï¼‰ï¼‰2 - 1

**çœ‹ cache_t æºç ç»“æ„ä¹‹å‰å»¶å±•å­¦ä¹  C++ åŸå­æ“ä½œ std::atomic<T> :**
## std::atomic<T>
std::atomic<T> æ¨¡ç‰ˆç±»å¯ä»¥ä½¿å¯¹è±¡æ“ä½œä¸ºåŸå­æ“ä½œï¼Œé¿å…å¤šçº¿ç¨‹ç«äº‰é—®é¢˜ã€‚
std::atomic<T> æ˜¯æ¨¡ç‰ˆç±»ï¼Œä¸€ä¸ªæ¨¡ç‰ˆç±»å‹ä¸º T çš„åŸå­å¯¹è±¡ä¸­å°è£…äº†ä¸€ä¸ªç±»å‹ä¸º T çš„å€¼ã€‚
`template <class T> struct atomic;`
åŸå­ç±»å‹å¯¹è±¡çš„ä¸»è¦ç‰¹ç‚¹å°±æ˜¯ä»ä¸åŒçº¿ç¨‹è®¿é—®ä¸ä¼šå¯¼è‡´æ•°æ®ç«äº‰ï¼ˆdata raceï¼‰ã€‚å› æ­¤ä»ä¸åŒçº¿ç¨‹è®¿é—®æŸä¸ªåŸå­å¯¹è±¡æ˜¯è‰¯æ€§ï¼ˆwell-definedï¼‰è¡Œä¸ºï¼Œè€Œé€šå¸¸å¯¹äºéåŸå­ç±»å‹è€Œè¨€ï¼Œå¹¶å‘è®¿é—®æŸä¸ªå¯¹è±¡ï¼ˆå¦‚æœä¸åšä»»ä½•åŒæ­¥æ“ä½œï¼‰ä¼šå¯¼è‡´æœªå®šä¹‰ï¼ˆundefinedï¼‰è¡Œä¸ºå‘ç”Ÿã€‚


## objc4-781 ç‰ˆæœ¬
**cache_t ç»“æ„**
```
struct cache_t {
#if CACHE_MASK_STORAGE == CACHE_MASK_STORAGE_OUTLINED
    /// æ–¹æ³•ç¼“å­˜æ•°ç»„ï¼ˆä»¥æ•£åˆ—è¡¨çš„å½¢å¼è¿›è¡Œå­˜å‚¨ï¼‰
    explicit_atomic<struct bucket_t *> _buckets; // å­˜å‚¨æ–¹æ³•ä¿¡æ¯ 8 
    /// å®¹é‡çš„ä¸´ç•Œå€¼ ï¼ˆæ•£åˆ—è¡¨é•¿åº¦ - 1ï¼‰
    explicit_atomic<mask_t> _mask; // ç•™äº†å¤šå°‘ä¸ªç¼“å­˜ä½ç½® 4
#elif CACHE_MASK_STORAGE == CACHE_MASK_STORAGE_HIGH_16
    explicit_atomic<uintptr_t> _maskAndBuckets;
    mask_t _mask_unused;
    
    // How much the mask is shifted by.
    static constexpr uintptr_t maskShift = 48;
    
    // Additional bits after the mask which must be zero. msgSend
    // takes advantage of these additional bits to construct the value
    // `mask << 4` from `_maskAndBuckets` in a single instruction.
    static constexpr uintptr_t maskZeroBits = 4;
    
    // The largest mask value we can store.
    static constexpr uintptr_t maxMask = ((uintptr_t)1 << (64 - maskShift)) - 1;
    
    // The mask applied to `_maskAndBuckets` to retrieve the buckets pointer.
    static constexpr uintptr_t bucketsMask = ((uintptr_t)1 << (maskShift - maskZeroBits)) - 1;
    
    // Ensure we have enough bits for the buckets pointer.
    static_assert(bucketsMask >= MACH_VM_MAX_ADDRESS, "Bucket field doesn't have enough bits for arbitrary pointers.");
#elif CACHE_MASK_STORAGE == CACHE_MASK_STORAGE_LOW_4
    // _maskAndBuckets stores the mask shift in the low 4 bits, and
    // the buckets pointer in the remainder of the value. The mask
    // shift is the value where (0xffff >> shift) produces the correct
    // mask. This is equal to 16 - log2(cache_size).
    explicit_atomic<uintptr_t> _maskAndBuckets;
    mask_t _mask_unused;

    static constexpr uintptr_t maskBits = 4;
    static constexpr uintptr_t maskMask = (1 << maskBits) - 1;
    static constexpr uintptr_t bucketsMask = ~maskMask;
#else
#error Unknown cache mask storage type.
#endif
    
#if __LP64__
    uint16_t _flags; // 2
#endif
    /// è¡¨ç¤ºå·²ç»ç¼“å­˜çš„æ–¹æ³•çš„æ•°é‡
    uint16_t _occupied; // å·²ç¼“å­˜çš„æ•°é‡ 2

public:
    static bucket_t *emptyBuckets();
    
    struct bucket_t *buckets();
    mask_t mask();
    mask_t occupied();
    void incrementOccupied();
    void setBucketsAndMask(struct bucket_t *newBuckets, mask_t newMask);
    void initializeToEmpty();

    unsigned capacity();
    bool isConstantEmptyCache();
    bool canBeFreed();

#if __LP64__
    bool getBit(uint16_t flags) const {
        return _flags & flags;
    }
    void setBit(uint16_t set) {
        __c11_atomic_fetch_or((_Atomic(uint16_t) *)&_flags, set, __ATOMIC_RELAXED);
    }
    void clearBit(uint16_t clear) {
        __c11_atomic_fetch_and((_Atomic(uint16_t) *)&_flags, ~clear, __ATOMIC_RELAXED);
    }
#endif

#if FAST_CACHE_ALLOC_MASK
    bool hasFastInstanceSize(size_t extra) const
    {
        if (__builtin_constant_p(extra) && extra == 0) {
            return _flags & FAST_CACHE_ALLOC_MASK16;
        }
        return _flags & FAST_CACHE_ALLOC_MASK;
    }

    size_t fastInstanceSize(size_t extra) const
    {
        ASSERT(hasFastInstanceSize(extra));

        if (__builtin_constant_p(extra) && extra == 0) {
            return _flags & FAST_CACHE_ALLOC_MASK16;
        } else {
            size_t size = _flags & FAST_CACHE_ALLOC_MASK;
            // remove the FAST_CACHE_ALLOC_DELTA16 that was added
            // by setFastInstanceSize
            return align16(size + extra - FAST_CACHE_ALLOC_DELTA16);
        }
    }

    void setFastInstanceSize(size_t newSize)
    {
        // Set during realization or construction only. No locking needed.
        uint16_t newBits = _flags & ~FAST_CACHE_ALLOC_MASK;
        uint16_t sizeBits;

        // Adding FAST_CACHE_ALLOC_DELTA16 allows for FAST_CACHE_ALLOC_MASK16
        // to yield the proper 16byte aligned allocation size with a single mask
        sizeBits = word_align(newSize) + FAST_CACHE_ALLOC_DELTA16;
        sizeBits &= FAST_CACHE_ALLOC_MASK;
        if (newSize <= sizeBits) {
            newBits |= sizeBits;
        }
        _flags = newBits;
    }
#else
    bool hasFastInstanceSize(size_t extra) const {
        return false;
    }
    size_t fastInstanceSize(size_t extra) const {
        abort();
    }
    void setFastInstanceSize(size_t extra) {
        // nothing
    }
#endif

    static size_t bytesForCapacity(uint32_t cap);
    static struct bucket_t endMarker(struct bucket_t *b, uint32_t cap);

    void reallocate(mask_t oldCapacity, mask_t newCapacity, bool freeOld);
    void insert(Class cls, SEL sel, IMP imp, id receiver);

    static void bad_cache(id receiver, SEL sel, Class isa) __attribute__((noreturn, cold));
};
```
```
struct bucket_t {
private:
    // IMP-first is better for arm64e ptrauth and no worse for arm64.
    // SEL-first is better for armv7and i386 and x86_64.
#if __arm64__
    explicit_atomic<uintptr_t> _imp; // è·å–æ–¹æ³•å®ç°
    explicit_atomic<SEL> _sel; // ä»¥æ–¹æ³•åä¸º keyï¼Œé€‰æ‹©å­
#else
    explicit_atomic<SEL> _sel;
    explicit_atomic<uintptr_t> _imp;
#endif
....
};
```
å®šä¹‰ LGPerson ç±»ï¼Œå¹¶å®ç°å‡ ä¸ªæ–¹æ³•å¹¶è°ƒç”¨ï¼š
```
// .h
@interface LGPerson : NSObject

- (void)instanceMethod1;
- (void)instanceMethod2;
- (void)instanceMethod3;
- (void)instanceMethod4;
- (void)instanceMethod5;
- (void)instanceMethod6;
- (void)instanceMethod7;

@end

// main.m ä¸­
LGPerson *person = [LGPerson alloc];
LGPerson *p = [person init]; // åœ¨æ­¤å¤„æ‰“æ–­ç‚¹

[p instanceMethod1];
[p instanceMethod2];
[p instanceMethod3];
[p instanceMethod4];
[p instanceMethod5];
[p instanceMethod6];
[p instanceMethod7];
```
é€šè¿‡æ§åˆ¶å°æ‰“å°:
```
// æ‰“å°ç±»ä¿¡æ¯
(lldb) p [person class]
(Class) $0 = LGPerson

// æ‹¿åˆ° cache ä¿¡æ¯
(lldb) x/4gx $0
0x1000021d8: 0x00000001000021b0 ï¼ˆisaï¼‰ 0x00000001003ee140 (superclass)
0x1000021e8: 0x0000000100677860 0x0002801000000003 (cache_t)
(lldb) p (cache_t *)0x1000021e8
(cache_t *) $1 = 0x00000001000021e8

// æŸ¥çœ‹ cache å…·ä½“æœ‰ä»€ä¹ˆä¸œè¥¿
(lldb) p *$1
(cache_t) $2 = {
  _buckets = {
    std::__1::atomic<bucket_t *> = 0x0000000100677860 {
      _sel = {
        std::__1::atomic<objc_selector *> = 0x00007fff70893e54
      }
      _imp = {
        std::__1::atomic<unsigned long> = 4041432
      }
    }
  }
  _mask = {
    std::__1::atomic<unsigned int> = 3
  }
  _flags = 32784
  _occupied = 2
}
```
è¿™é‡Œå°±å¥‡æ€ªäº†ï¼Œæ˜æ˜æ²¡æœ‰è°ƒç”¨ä»»ä½•å®ä¾‹æ–¹æ³•ï¼Œä¸ºä»€ä¹ˆå·²ç»ç¼“å­˜äº†ä¸¤ä¸ªäº†å‘¢ï¼Ÿ
ç»§ç»­æ‰“å° _buckets ä¿¡æ¯ï¼š
```
(lldb) p (bucket_t *)$1.buckets()
(bucket_t *) $3 = 0x0000000100677860
  Fix-it applied, fixed expression was: 
    (bucket_t *)$1->buckets()
(lldb) p (bucket_t *)$1->buckets()
(bucket_t *) $4 = 0x0000000100677860
(lldb) p $4[0]
(bucket_t) $5 = {
  _sel = {
    std::__1::atomic<objc_selector *> = 0x00007fff70893e54
  }
  _imp = {
    std::__1::atomic<unsigned long> = 4041432
  }
}
(lldb) p $4[1]
(bucket_t) $6 = {
  _sel = {
    std::__1::atomic<objc_selector *> = 0x0000000000000000
  }
  _imp = {
    std::__1::atomic<unsigned long> = 0
  }
}
(lldb) p $4[2]
(bucket_t) $7 = {
  _sel = {
    std::__1::atomic<objc_selector *> = 0x0000000000000000
  }
  _imp = {
    std::__1::atomic<unsigned long> = 0
  }
}
(lldb) 
```
> åªçœ‹åˆ° `$4[0]` å‘ç°ç¼“å­˜çš„æ–¹æ³•ï¼Œä½†æ˜¯æ— æ³•çœ‹å‡ºç©¶ç«Ÿæ˜¯ç¼“å­˜çš„ä»€ä¹ˆæ–¹æ³•ã€‚

åœ¨åˆå§‹åŒ– LGPerson ä¹‹å‰ï¼Œé€šè¿‡ `Class cls = NSClassFromString(@"LGPerson");` æ–¹å¼å¾—åˆ° clsï¼Œæ‰“å° cls é‡Œé¢çš„ç¼“å­˜ä¿¡æ¯ï¼Œå‘ç° `mask = 0, _occupied = 0`ï¼Œç„¶å `LGPerson *person = [LGPerson alloc];` åˆå§‹åŒ–ä¹‹åï¼Œå†æ‰“å°ï¼Œå‘ç° `mask = 3, _occupied = 2`ï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨ alloc è¿‡ç¨‹ä¸­ï¼Œè¿›è¡Œäº†ç¼“å­˜æ“ä½œã€‚

æ–­ç‚¹å¾€ä¸‹èµ°ï¼Œè°ƒç”¨äº†ä¸€ä¸ªå®ä¾‹æ–¹æ³• `[person init]`ï¼Œè°ƒç”¨ `init` ä¹‹åï¼Œå†æ‰“å° `cache` ä¿¡æ¯ï¼Œçœ‹åˆ° `init` è¢«ç¼“å­˜åˆ°äº† `LGPerson` ä¸­ï¼Œè¿™ä¹Ÿè¯æ˜äº† `å³ä½¿æ˜¯çˆ¶ç±»çš„æ–¹æ³•ï¼Œä¹Ÿä¼šç¼“å­˜åœ¨æœ¬ç±»ä¸­`ã€‚

```
// ä»»ä½•å‡½æ•°æœªæ‰§è¡Œæ—¶ï¼Œåªè°ƒç”¨ NSClassFromString(@"LGPerson")
Class cls = NSClassFromString(@"LGPerson");
// æ‰“å°:
(cache_t) $3 = {
  _buckets = {
    std::__1::atomic<bucket_t *> = 0x00000001003e8490 {
      _sel = {
        std::__1::atomic<objc_selector *> = 0x0000000000000000
      }
      _imp = {
        std::__1::atomic<unsigned long> = 0
      }
    }
  }
  _mask = {
    std::__1::atomic<unsigned int> = 0 // ä¸´ç•Œå®¹é‡ 0
  }
  _flags = 16
  _occupied = 0 // å·²å ç”¨ 0 
}

// æ‰§è¡Œåˆ° [persont init] å¤„
// å‘½ä»¤åˆ—è¡¨
p [person class]
x/4gx $0
p (cache_t *)0x1000021f0
p *$1

...
_mask = {
  std::__1::atomic<unsigned int> = 3 // ä¸´ç•Œå®¹é‡æ˜¯ 3
}
_flags = 32784
_occupied = 2 // å·²å ç”¨æ˜¯ 2
...
// æ‰§è¡Œåˆ°å®Œ init å
...
_mask = {
  std::__1::atomic<unsigned int> = 3 // ä¸´ç•Œå®¹é‡æ˜¯ 3
}
_flags = 32784
_occupied = 3 // å·²å ç”¨æ˜¯ 3
...

// ä½¿ç”¨ä¸Šé¢ cls 
// æ‰“å° _mask = 3 _occupied = 3
```

| è°ƒç”¨è¿‡å®ä¾‹æ–¹æ³•æ•°é‡ |0|1|2|3|4|5|6|7|8|
|---|---|---|---|---|---|---|---|---|---|
|_mask|3|3|7|7|7|7|7|7|15|
|_occupied|2|3|1|2|3|4|5|6|1|

### ä»æºç çœ‹ç¼“å­˜æœºåˆ¶
ä»å“ªé‡Œå…¥æ‰‹å‘¢ï¼Ÿå…ˆçœ‹ mask æ˜¯æ€ä¹ˆæ¥çš„
åœ¨ cache_t  ä¸­æœ‰ä¸€ä¸ª mask() çš„æ–¹æ³•ï¼Œé‚£å°±æ‰¾è¿™ä¸ªæ–¹æ³•
```
typedef uint32_t mask_t; // mask_t æ˜¯æ— ç¬¦å· 32 ä½ intï¼Œå  4 ä¸ªå­—èŠ‚

// memory_order::memory_order_relaxed
// Relaxed ordering: åœ¨å•ä¸ªçº¿ç¨‹å†…ï¼Œæ‰€æœ‰åŸå­æ“ä½œæ˜¯é¡ºåºè¿›è¡Œçš„ã€‚æŒ‰ç…§ä»€ä¹ˆé¡ºåºï¼ŸåŸºæœ¬ä¸Šå°±æ˜¯ä»£ç é¡ºåºï¼ˆsequenced-beforeï¼‰ã€‚è¿™å°±æ˜¯å”¯ä¸€çš„é™åˆ¶äº†ï¼ä¸¤ä¸ªæ¥è‡ªä¸åŒçº¿ç¨‹çš„åŸå­æ“ä½œæ˜¯ä»€ä¹ˆé¡ºåºï¼Ÿä¸¤ä¸ªå­—ï¼šä»»æ„ã€‚

mask_t cache_t::mask() 
{
    return _mask.load(memory_order::memory_order_relaxed);
}

```
```
// objc-cache.mm æ–‡ä»¶å¼€å¤´æ³¨é‡Š
 Method cache locking (GrP 2001-1-14)

For speed, objc_msgSend does not acquire any locks when it reads method caches.
ä¸ºäº†æé«˜é€Ÿåº¦ï¼Œobjc_msgSendåœ¨è¯»å–æ–¹æ³•ç¼“å­˜æ—¶ä¸è·å–ä»»ä½•é”ã€‚

Instead, all cache changes are performed so that any objc_msgSend running concurrently with the cache mutator will not crash or hang or get an incorrect result from the cache.
ç›¸åï¼Œå°†æ‰§è¡Œæ‰€æœ‰é«˜é€Ÿç¼“å­˜æ›´æ”¹ï¼Œä»¥ä¾¿ä¸é«˜é€Ÿç¼“å­˜æ›´æ”¹å™¨åŒæ—¶è¿è¡Œçš„ä»»ä½•objc_msgSendéƒ½ä¸ä¼šå´©æºƒæˆ–æŒ‚èµ·ï¼Œæˆ–ä»é«˜é€Ÿç¼“å­˜ä¸­è·å–é”™è¯¯çš„ç»“æœã€‚

When cache memory becomes unused (e.g. the old cache after cache expansion), it is not immediately freed, because a concurrent objc_msgSend could still be using it.
å½“ç¼“å­˜å†…å­˜æœªä½¿ç”¨æ—¶ï¼ˆä¾‹å¦‚ï¼Œç¼“å­˜æ‰©å±•åçš„æ—§ç¼“å­˜ï¼‰ï¼Œå®ƒä¸ä¼šç«‹å³è¢«é‡Šæ”¾ï¼Œå› ä¸ºå¹¶å‘objc_msgSendå¯èƒ½ä»åœ¨ä½¿ç”¨å®ƒã€‚

Instead, the memory is disconnected from the data structures and placed on a garbage list.
è€Œæ˜¯å°†å†…å­˜ä¸æ•°æ®ç»“æ„æ–­å¼€è¿æ¥ï¼Œå¹¶å°†å…¶æ”¾ç½®åœ¨åƒåœ¾æ¸…å•ä¸Šã€‚

The memory is now only accessible to instances of objc_msgSend that were running when the memory was disconnected; any further calls to objc_msgSend will not see the garbage memory because the other data structures don't point to it anymore.
ç°åœ¨ï¼Œåªæœ‰æ–­å¼€å†…å­˜æ—¶æ­£åœ¨è¿è¡Œçš„objc_msgSendå®ä¾‹æ‰èƒ½è®¿é—®è¯¥å†…å­˜ï¼›è¿›ä¸€æ­¥è°ƒç”¨objc_msgSendå°†çœ‹ä¸åˆ°åƒåœ¾å†…å­˜ï¼Œå› ä¸ºå…¶ä»–æ•°æ®ç»“æ„ä¸å†æŒ‡å‘è¯¥å†…å­˜ã€‚

The collecting_in_critical function checks the PC of all threads and returns FALSE when all threads are found to be outside objc_msgSend.
collect_in_critical å‡½æ•°æ£€æŸ¥ PC çš„æ‰€æœ‰çº¿ç¨‹ï¼Œå¹¶åœ¨å‘ç°æ‰€æœ‰çº¿ç¨‹ä¸åœ¨objc_msgSendä¹‹å¤–æ—¶è¿”å›FALSEã€‚

This means any call to objc_msgSend that could have had access to the garbage has finished or moved past the cache lookup stage, so it is safe to free the memory.
è¿™æ„å‘³ç€å¯¹objc_msgSendçš„æ‰€æœ‰æœ¬å¯ä»¥è®¿é—®åƒåœ¾çš„è°ƒç”¨éƒ½å·²å®Œæˆæˆ–ç§»åˆ°äº†é«˜é€Ÿç¼“å­˜æŸ¥æ‰¾é˜¶æ®µï¼Œå› æ­¤å¯ä»¥å®‰å…¨åœ°é‡Šæ”¾å†…å­˜ã€‚

All functions that modify cache data or structures must acquire the cacheUpdateLock to prevent interference from concurrent modifications.
æ‰€æœ‰ä¿®æ”¹ç¼“å­˜æ•°æ®æˆ–ç»“æ„çš„å‡½æ•°éƒ½å¿…é¡»è·å– cacheUpdateLockï¼Œä»¥é˜²æ­¢å¹¶å‘ä¿®æ”¹é€ æˆå¹²æ‰°ã€‚

The function that frees cache garbage must acquire the cacheUpdateLock and use collecting_in_critical() to flush out cache readers.
é‡Šæ”¾ç¼“å­˜åƒåœ¾çš„å‡½æ•°å¿…é¡»è·å– cacheUpdateLock å¹¶ä½¿ç”¨ collection_in_criticalï¼ˆï¼‰åˆ·æ–°ç¼“å­˜è¯»å–å™¨ã€‚

The cacheUpdateLock is also used to protect the custom allocator used for large method cache blocks.
cacheUpdateLockè¿˜ç”¨äºä¿æŠ¤ç”¨äºå¤§å‹æ–¹æ³•ç¼“å­˜å—çš„è‡ªå®šä¹‰åˆ†é…å™¨ã€‚

Cache readers (PC-checked by collecting_in_critical())
objc_msgSend*
cache_getImp

Cache writers (hold cacheUpdateLock while reading or writing; not PC-checked)
cache_fill         (acquires lock)
cache_expand       (only called from cache_fill) 
cache_create       (only called from cache_expand)
bcopy               (only called from instrumented cache_expand)
flush_caches        (acquires lock)
cache_flush        (only called from cache_fill and flush_caches)
cache_collect_free (only called from cache_expand and cache_flush)

UNPROTECTED cache readers (NOT thread-safe; used for debug info only)
cache_print

_class_printMethodCaches
_class_printDuplicateCacheEntries
_class_printMethodCacheStatistics

```
å…¥å£ï¼Œ`cache_fill`:
```
void cache_fill(Class cls, SEL sel, IMP imp, id receiver)
{
    runtimeLock.assertLocked();

#if !DEBUG_TASK_THREADS
    // Never cache before +initialize is done
    if (cls->isInitialized()) {
        cache_t *cache = getCache(cls);
// #if CONFIG_USE_CACHE_LOCK
//        mutex_locker_t lock(cacheUpdateLock);
// #endif
        cache->insert(cls, sel, imp, receiver);
    }
// #else
//    _collecting_in_critical();
// #endif
}

cache_t *getCache(Class cls) 
{
    ASSERT(cls);
    return &cls->cache;
}

// æ­¥éª¤ 1 æ’å…¥ç¼“å­˜ä¿¡æ¯

ALWAYS_INLINE
void cache_t::insert(Class cls, SEL sel, IMP imp, id receiver) // æ’å…¥ç¼“å­˜
{
#if CONFIG_USE_CACHE_LOCK
    cacheUpdateLock.assertLocked();
#else
    runtimeLock.assertLocked();
#endif

    ASSERT(sel != 0 && cls->isInitialized());

    // Use the cache as-is if it is less than 3/4 full
    // å¦‚æœç¼“å­˜æœªæ»¡ 3/4ï¼Œåˆ™æŒ‰åŸæ ·ä½¿ç”¨å®ƒ ï¼ˆæŒ‰ç…§ç¬¬ä¸€æ¬¡ç¼“å­˜æ¥èµ°æµç¨‹ï¼‰
    // occupied() = 0 newOccupied = 1
    mask_t newOccupied = occupied() + 1;
    
    unsigned oldCapacity = capacity(), capacity = oldCapacity;
    
    if (slowpath(isConstantEmptyCache())) { // å¦‚æœç¼“å­˜æ˜¯ç©ºçš„ï¼Œè¿™é‡Œå¾ˆå¯èƒ½ä¸º å‡ï¼Œç„¶åç”¨çš„ slowpath
        // Cache is read-only. Replace it. // å› ä¸ºç¼“å­˜æ˜¯åªè¯»ï¼Œç›´æ¥æ›¿æ¢
        if (!capacity) capacity = INIT_CACHE_SIZE;
        reallocate(oldCapacity, capacity, /* freeOld */false);
    }
    else if (fastpath(newOccupied + CACHE_END_MARKER <= capacity / 4 * 3)) {
        // Cache is less than 3/4 full. Use it as-is.
    }
    else {
        capacity = capacity ? capacity * 2 : INIT_CACHE_SIZE;
        if (capacity > MAX_CACHE_SIZE) {
            capacity = MAX_CACHE_SIZE;
        }
        reallocate(oldCapacity, capacity, true);
    }

    bucket_t *b = buckets();
    mask_t m = capacity - 1;
    mask_t begin = cache_hash(sel, m);
    mask_t i = begin;

    // Scan for the first unused slot and insert there.
    // There is guaranteed to be an empty slot because the
    // minimum size is 4 and we resized at 3/4 full.
    do {
        if (fastpath(b[i].sel() == 0)) {
            incrementOccupied();
            b[i].set<Atomic, Encoded>(sel, imp, cls);
            return;
        }
        if (b[i].sel() == sel) {
            // The entry was added to the cache by some other thread
            // before we grabbed the cacheUpdateLock.
            return;
        }
    } while (fastpath((i = cache_next(i, m)) != begin));

    cache_t::bad_cache(receiver, (SEL)sel, cls);
}


```


**å‚è€ƒé“¾æ¥:ğŸ”—**
[C++11 å¹¶å‘æŒ‡å—å…­( <atomic> ç±»å‹è¯¦è§£äºŒ std::atomic )](https://www.cnblogs.com/haippy/p/3301408.html)
[å¦‚ä½•ç†è§£ C++11 çš„å…­ç§ memory orderï¼Ÿ](https://www.zhihu.com/question/24301047)





